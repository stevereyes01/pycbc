###############################################################################
PyGRB: A GRB triggered CBC analysis workflow generator
###############################################################################

===============
Introduction
===============

PyGRB is a tool used to analyse data from multiple detectors coherently and
then perform various signal-based veto cuts and data quality cuts to determine
whether or not a compact binary coalescence signal is present in the given data
coming from the same point in the sky and at the same time as an observed short
GRB.

The output is a webpage containing the plots that can be used to understand the
results of the analysis.

.. _howtorunpygrb:

=======================
How to run
=======================

Here we document the stages needed to run the triggered coherent GRB search.

-------------------------------------
Copy pygrb.py into your run directory
-------------------------------------

Now copy the workflow generation script into your run directory::

    RUN_DIRECTORY=path/to/run_directory
    mkdir -p ${RUN_DIRECTORY}
    cd ${RUN_DIRECTORY}
    cp /src/pycbc/examples/workflow/pygrb/pygrb.py .

----------------------------------------------------------------------------
The configuration file - Do you already have configuration (.ini) file(s)?
----------------------------------------------------------------------------
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
Yes, I already have configuration files
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Great! Then copy the configuration files into your run directory::

    cp /path/to/config_file1.ini /path/to/config_file2.ini .

and set the names of these configuration files in your path. If you have more
than one configuration file they must be space separated::

    LOCAL_CONFIG_FILES="config_file1.ini config_file2.ini"

Now go down to :ref:`pygrbgenerate`.

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
No, I need to edit a configuration file
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

The default configuration files is found in::

    /src/pycbc/examples/workflow/pygrb/pygrb.ini

This file contains all the details needed to construct a pyGRB workflow

.. note::

    If you are unfamiliar with pycbc workflows, look through these files.
    
* pygrb.ini contains options that are used when running the pycbc.workflow
  parts of the workflow

The pygrb.ini example is set up to run on S6 data and analysing only H1 and L1.

If you want to run in this default configuration please jump down to
:ref:`pygrbgenerate`.

If you want to run on non-S6 data, analyze a different set of ifos, or change
any data-type or segment length options, you will have to edit some additional
options::

    [workflow]
    h1-channel-name = H1:LDAS-STRAIN
    l1-channel-name = L1:LDAS-STRAIN

    [workflow-ifos]
    ; This is the list of ifos to analyse
    h1 =
    l1 =

    [workflow-datafind]
    datafind-h1-frame-type = H1_LDAS_C02_L2
    datafind-l1-frame-type = L1_LDAS_C02_L2

    [workflow-segments]
    segments-H1-science-name = H1:DMT-SCIENCE:4
    segments-L1-science-name = L1:DMT-SCIENCE:4
    segments-V1-science-name = V1:ITF_SCIENCEMODE
    segments-database-url = https://segdb.ligo.caltech.edu
    segments-veto-definer-url = https://www.lsc-group.phys.uwm.edu/ligovirgo/cbc/public/segments/S6/H1L1V1-S6_CBC_LOWMASS_B_OFFLINE-937473702-0.xml

    [workflow-exttrig_segments]
    ; options for the coherent search (development)
    on-before = 1
    on-after = 5
    min-before = 60
    min-after = 60
    min-duration = 256
    max-duration = 4096
    pad-data = 8
    quanta = 128

    ALL the [tisi], [tisi-zerolag], [tisi-slides] sections (potentially)

To run through this

* The [workflow-ifos] section supplies which ifos will be analysed if data is
  found and available
* The [workflow-exttrig_segments] section supplies the GRB search-specific
  options for the data segment to be analyzed
* The X1-channel-name options are the h(t) channel name in the frames
* The datafind-X1-frame-type is the type of the frames for use when calling
  gw_data_find
* The segments-X1-science-name is the flag used to store science times in the
  segment database
* segments-database-url points to the segment database
* segments-veto-definer-url points to the url where the veto-definer file can
  be found.

The remaining options affect how the jobs run, these should not be edited
unless you know what you are doing ... but can freely be added if you do know
what you are doing and want to change something. To find out more details about
the possible options for any stage of the workflow, follow the links at
:ref:`workflowhomepage`.

Now you have a configuration file and can follow the same instructions as
above. That is: 

Copy the configuration file into your run directory::

    cp /path/to/pygrb.ini .

and set the name of the configuration file in your path. If you have more than
one configuration file they must be space separated::

    LOCAL_CONFIG_FILES="pygrb.ini"

.. _pygrbgenerate:

-----------------------
Generate the workflow
-----------------------

When you are ready, you can generate the workflow. This may be done by setting
a number of variables in your environment before launching the generation
script.

First we need to choose a trigger time, ie. the GPS Earth-crossing time
of the GRB signal. You should also set the GRB name. For example::

    GRB_TIME=969675608
    GRB_NAME=100928A

We should next set the sky coordinates of the GRB in RA and Dec, in this
example::

    RA=223.0
    DEC=-28.5

If you are using a pregenerated template bank and do not have a path to the
bank set in your config file, set it here::

    BANK_FILE=path/to/templatebank

You also need to specify the directory for storing log files.

 * For CIT,LHO,LLO or SYR set::

    export LOGPATH=/usr1/${USER}/log
    export PIPEDOWNTMPSPACE=/usr1/${USER}
    mkdir -p $LOGPATH

 * For Atlas set::

    export LOGPATH=/local/user/${USER}/log/
    export PIPEDOWNTMPSPACE=/local/user/${USER}
    mkdir -p $LOGPATH 

 * For UWM set::

    export LOGPATH=/people/${USER}/log/
    export PIPEDOWNTMPSPACE=/localscratch/${USER}
    mkdir -p $LOGPATH

 * On the TACC XSEDE cluster, it is recommended to store your ihope directory under the work filesystem.
   For the TACC XSEDE cluster set::

    export LIGO_DATAFIND_SERVER=tacc.ligo.org:80
    export LOGPATH=${SCRATCH}/log
    export PIPEDOWNTMPSPACE=/tmp
    mkdir -p $LOGPATH

You also need to choose where the html results page will be generated. For
example::

    export HTMLDIR=/home/${USER}/public_html/pygrb

If you are using locally editted or custom configuration files then you can
create the workflow from within the run directory using::

    pygrb.py --local-config-files ${LOCAL_CONFIG_FILES} \
             --config-overrides workflow:ra:${RA} \
                                workflow:dec:${DEC} \
                                workflow:trigger-name:${GRB_NAME} \
                                workflow:trigger-time:${GRB_TIME} \
                                workflow:start-time:$(( GRB_TIME - 4096 )) \
                                workflow:end-time:$(( GRB_TIME + 4096 )) \
                                workflow-tmpltbank:tmpltbank-pregenerated-bank:${BANK_FILE}
.. _pygrbplan:

-----------------------------------------
Planning and Submitting the Workflow
-----------------------------------------
CD into the directory where the dax was generated::

    cd GRB${GRB_NAME}

From the directory where the dax was created, run the planning script::

    pycbc_basic_pegasus_plan pygrb.dax $LOGPATH

Submit the workflow by following the instructions at the end of the script
output, which looks something like::

    pegasus-run  /path/to/analysis/run

-------------------------------------------------------------------------------------------------------------------------------------------
Monitor and Debug the Workflow (`Detailed Pegasus Documentation <https://pegasus.isi.edu/wms/docs/latest/tutorial.php#idm78622034400>`_)
-------------------------------------------------------------------------------------------------------------------------------------------

To monitor the above workflow, one would run::

    pegasus-status /path/to/analysis/run
    
To get debugging information in the case of failures.::

    pegasus-analyzer /path/to/analysis/run

=============================
Workflow visualization
=============================

-----------------------------
Pegasus Dashboard
-----------------------------

The `pegeasus dashboard <http://pegasus.isi.edu/wms/docs/latest/ch02s11.php>`_
is a visual and interactive way to get information about the progress, status,
etc of your workflows.

The software can be obtained from a seprate pegasus package here
<https://github.com/pegasus-isi/pegasus-service>.

-----------------------------
Pegasus Plots
-----------------------------


Pegasus has a tool called pegasus-plan to visualize workflows. To generate
these charts and create an summary html page with this information, one would
run::

    export PPLOTSDIR=${HTMLDIR}/pegasus_plots
    pegasus-plots --plotting-level all --output ${PPLOTSDIR} /path/to/analysis/run

The Invocation Breakdown Chart section gives a snapshot of the workflow. You
can click on the slices of the pie chart and it will report the number of
failures, average runtime, and max/min runtime for that type of jobs in the
workflow. The radio button labeled runtime will organize the pie chart by total
runtime rather than the total number of jobs for each job type.

The Workflow Execution Gantt Chart section breaks down the workflow how long it
took to run each job. You can click on a job in the gantt chart and it will
report the job name and runtime.

The Host Over Time Chart section displays a gantt chart where you can see what
jobs in the workflow ran on a given machine.

.. _pygrbreuse:

================================
Reuse of workflow file products
================================

One of the features of  Pegasus is to reuse the data products of prior runs.
This can be used to expand an analysis or recover a run with mistaken settings
without duplicating work.

-----------------------------------------
Generate the full workflow you want to do
-----------------------------------------

First generate the full workflow for the run you would like to do as normal,
following the instructions of this page from :ref:`howtorunpygrb`, but stop
before planning the workflow in :ref:`pygrbplan`.

-----------------------------------------------------
Select the files you want to reuse from the prior run
-----------------------------------------------------

Locate the directory of the run that you would like to reuse. There is a file
called GRB${GRB_NAME}/output.map, that contains a listing of all of the data
products of the prior workflow.

Select the entries for files that you would like to skip generating again and
place that into a new file. The example below selects all the inspiral and 
tmpltbank jobs and places their entries into a new listing called
prior_data.map.::

    # Lets get the tmpltbank entries
    cat /path/to/old/run/GRB${GRB_NAME}/output.map | grep 'TMPLTBANK' > prior_data.map
    
    # Add in the inspiral  files
    cat /path/to/old/run/GRB${GRB_NAME}/output.map | grep 'INSPIRAL' >> prior_data.map

.. note::

    You can include files in the prior data listing that wouldn't be generated
    anyway by your new run. These are simply ignored.

Place this file in the GRB${GRB_NAME}/  directory of your new run.

---------------------------
Plan the workflow
---------------------------

From the directory where the dax was created, run the planning script::

    pycbc_basic_pegasus_plan pygrb.dax $LOGPATH --cache prior_data.map

Follow the remaining :ref:`pygrbplan` instructions to submit your reduced
workflow.

